---
title: "Replication of Effects of Self-Explaining on Learning and Transfer of Critical Thinking Skills
X by Van Peppen et al. (2018, November)"
author: "Van Peppen, L. M., Verkoeijen, P. P., Heijltjes, A. E., Janssen, E. M., Koopmans, D., & Van Gog, T.
date: "`r format(Sys.time(), '%11 %18, %2018')`"
format: 
  html: "https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2018.00100/full 
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. --> 

## Introduction
Reflecting upon my project proposal feedback, I would like to continue replicating Van Peppen and colleagues (2018) study. The study aims to examine how generating explanations for oneself (i.e. self explanations) fosters critical thinking. Though this does not directly relate to my first year project, I am still interested in launching a study where I test how different types of explanation strategies (such as AI-generated vs self-explanations) shapes the way learners’ retain and critically think about science. Finally, replicating this study will balance both the challenge and feasibility of coding and familiarizing myself with R. Through these efforts, I hope to deepen my understanding on how different learning strategies foster critical thinking and better learning outcomes.

## Methods

### Power Analysis
Calculated a power analysis, in which the study computed that 79 participants would be sufficient for the study. They computed the power analysis using a 3x2x2 mixed ANOVA model, with alpha of 0.05 and correlation of 0.3, and sample size of N=79. They found that this sample size would be sufficient for picking up medium sized effects.

### Planned Sample
88 UC San Diego psychology undergraduate students will be recruited for this replication project. Participants will be recruited  through the Sona-system. 

### Materials
For both the pretest and posttest, participants will be administered the critical thinking (CT) skills test. The CT-skills test tried to measure critical thinking skills in two key categories. The first category was to understand how well people were able to logically reason. To test this, they implemented two exams that measured how susceptible participants are to believe in false information (Syllogistic Reasoning task) as well as how likely participants verify versus disprove claims (Wason Selection task). The second category the study aimed to assess was participants’ statistical reasoning skills. The first test they used was the Base-rate tasks to evaluate whether people prioritized anecdotal evidence over statistical findings. Additionally, they integrated the Conjunction tasks which measured how likely people bias statistics: such as believing events described in detail are more probable than a simplistic recount. 

For the intervention phase, the study provided CT-instructions that educated participants on what inductive versus deductive reasoning is. Additionally, the instructions integrated two worked examples per exam they were administered in the pretest. Then, participants practiced the learned skills with CT-practice material that allowed participants to apply their skills to four different scenarios, and explain why they came to their conclusions. With each test administered, participants were given a mental effort survey where they rated on a 9-point Likert scale (1 being extremely low effort to 9 being extremely high effort) on how much cognitive effort it took to complete the task.

### Procedure	
Participants signed the informed consent form before completing the pretest that measured their initial critical thinking skills through Sona-systems. Afterwards, they were given a paper packet of the CT-instruction, moving them to the intervention phase that taught participants different types of reasoning skills. Then, participants integrated what they learned by solving four practice problems by engaging in self-explanation. Once they completed the practice intervention, participants completed the immediate posttest. Two weeks later, participants were given a delayed posttest that involved the same questions as the immediate version. However, they were also tasked to rate how much mental effort was exerted in completing the questions. 

### Analysis Plan



Expressed how "for all analyses in this paper a p-value of 0.05 was used as a threshold for statistical signficance. Partial eta-squared is reported as a measure of effect size for the ANOVAs, for which 0,01 is considered small, 0.06 medium, and 0.14 large, and Cohen's d is reported for the post-hoc tests, with values of 0.2, 0.5, and 0.8 representing a small, medium, and large effect size respectively (Cohen, 1988)"

**Clarify key analysis of interest here** The key analysis of interest is seeing the interaction between self-explanations and learning outcomes as well as transfer performance. It will be calculated through a mixed ANOVA. 

### Differences from Original Study
One major concern of replicating this study is conducting a longtitudinal study. Addressing this feedback, I am planning to not add the delayed posttest portion of the study. The rationale behind this decision stems from the delayed posttest just measuring how well a student retained information from the task. The immediate posttest will be sufficient to compare how the CT training and self-explanations affected learning outcomes. Beyond this, other concerns raised was understanding how participants' engagement of the study would be impacted if moved to online. One way to test engagement is creating a survey at the end asking if they honestly used any distractions to account for transparent barriers in the study. Also, calculating a power analysis in R to find the amount of participants needed to get medium sized effects will be the next step. The final solution addresses the third feedback: obtaining the CT materials. By emailing the authors, I am hoping to also ask them whether adding all four CT measures are necessary in measuring critical thinking. If it is not, I will utilize their feedback and make the study go faster. This way, it can hopefully keep participants a little more engaged.

### Reliability & Validity
The study scores reliability through recruiting two raters to score the data. Since interrater reliability was high, two raters only scored 25% of the tests (r=0.899) and one rater scored the rest of the CT skills tests. Beyond reliability, the study fails to have ecologically validity, as Vanpeppen and colleagues (2018) admit that the study may be difficult to translate into classroom environments. To elaborate, the study could not find any meaningful effects on how self-explanations benefit unbiased reasoning skills. This is due to how those who attend class are more likely to be motivated to do well on tasks, which make the perceived difficulty of the task less effective.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
The study recruited 88 participants, but 11 people were taken out of the data due to technical issues or partially reported data. In the replication project, this will not be an issue as there is no delayed posttest. Therefore, the data of 79 participants were analyzed.

#### Differences from pre-data collection methods plan
Before data collection, the only differences would be to take out the delayed posttest from the study. 


## Results


### Data preparation

To prepare my data, I will launch my study on jsPsych to ensure that data analyses goes smooth as possible. I will label all my variables based on the CT skills test, as well as the pretest and immediate posttest. If there is any incomplete data, I will remove the participant out of the dataset. This will not be a huge issue, as there is no delayed posttest which would decrease the chance of this happening. I will also preregister my replication study to ensure that I remain open about my practices.  
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

Mixed ANOVAs analyses were heavily conducted to test the performance gains on MC-answers. Though the original study employed a 3x2 design, I will analyze the data through a mixed ANOVA 2x2 design. Specifically, I will analyze self-explanation effectiveness and timing (which would compare pretest and immediate posttest). I will be calculating the p-value, effect size (through Cohen's d), and 95% confidence intervals.

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
