---
title: "Replication of Vanpeppen et al. (2018) Data Analysis"
author: "Christine Lee"
date: "last updated at 11.29.2025"
format: 
  html:
    code-fold: false
    toc: true
    toc-depth: 3
    theme: cosmo
editor: visual
---

# Introduction

\[add brief introduction of what the project will be about\]

# Setup

```{r}
# Load required packages
library(tidyverse) 
library(knitr)
library(tidyr)
library(dplyr)
library(kableExtra)
library(jsonlite)
library(afex)
library(ggplot2)
library(Hmisc)
library(emmeans)

# Set theme for plots
theme_set(theme_minimal(base_size = 12))

# Set random seed for reproducibility
set.seed(123)
```

# Data Loading

In this section, we are loading in the data folder containing all the csvs from the study. Then, we will combine all the csvs from each folder and merge them together into one data set.

```{r}
# Labeling the data folders that will be analyzed in this study
data_folder1 <- "/Users/christinelee/Documents/vanpeppen2018/sona_data"
data_folder2 <- "/Users/christinelee/Documents/vanpeppen2018/missing_sona id"
# Merge all the csv files into ONE data set per individual folder
load_folder <- function(folder_path) {
  files <- list.files(folder_path,
                      pattern = "\\.csv$",
                      full.names = TRUE)

  if (length(files) == 0) {
    stop(paste("No CSV files found in", folder_path))
  }

  do.call(rbind, lapply(files, function(f) {
    tmp <- read.csv(f)
    tmp$source_file <- basename(f)
    tmp
  }))
}
# Load all the pilot data csvs into one dataset
dataset1 <- load_folder(data_folder1)
dataset2 <- load_folder(data_folder2)
# combine dataset 1 and dataset 2 together in one csv
all_data <- rbind(dataset1, dataset2)
#check
dim(all_data)      # rows x columns
head(all_data)
table(all_data$source_file)  # how many rows from each csv
```

# Pre-processing Data

Once the data in the file are all merged together, now we need to filter out the participants that failed the attention check. Since there is an unbalanced amount of participants, we will randomly select 40 participants per condition to equally analyze the data.

## Attention Check Filtering

Will exclude all the participants that failed the attention check. This will ensure that there is no confound in the data.

```{r}
# Reload merged data
data_base <- all_data
# Score attention checks as true/false
data_base$attention_question_yes <-
  data_base$attention_question_passed == "true"
data_base$attention_effort_yes <-
  data_base$attention_effort_passed == "true"
# Filter the rows where the attention checks are true
passing_rows <- data_base [
  data_base$attention_question_yes == TRUE &
    data_base$attention_effort_yes == TRUE,
]
# Check how many rows passed the attention check
nrow(passing_rows)
# Look at the csv files that passed the attention checks
head(passing_rows[, c("source_file",
                      "attention_question_passed",
                      "attention_effort_passed",
                      "attention_question_yes",
                      "attention_effort_yes")])

# Select the source files that participants' passed.
passed_ids <- unique(passing_rows$source_file)
passed_ids
length(passed_ids)
# Finally, keep all the rows except those who have failed the attention check
data_attention <- data_base [
  data_base$source_file %in% passed_ids,
]
```

## Randomization 

Before we format the question, we will randomly select 40 participants from each condition (SE vs no SE) to analyze. This will amount to a total of 80 participants.

```{r}
# Split between the self-explanation versus no self-explanation participants.
data_attention <- data_attention[
  data_attention$condition %in% c("self_explanation", "no_self_explanation"),
]
# Obtain a row per participant in their respective conditions
participants <- unique(data_attention[, c("source_file", "condition")])
sampled_participants <- participants %>%
  group_by(condition) %>%
  sample_n(size = 40) %>%   # because both groups have at least 40
  ungroup()
print(sampled_participants)
# Take the full data of those participants
sampled_ids <- sampled_participants$source_file
data_sampled <- data_attention %>%
  filter(source_file %in% sampled_ids)
# Number of unique participants = 80
print(length(unique(data_sampled$source_file)))
# Should show 40 in each condition
participants_sampled <- unique(data_sampled[, c("source_file", "condition")])
print(table(participants_sampled$condition))
# Peek at full sampled dataset
head(data_sampled)
```

# Format Questions

In this section, I will format the csv files so that they can be easily analyzed and scored. Each section will isolate the desired timing condition and organized to accurately show all the questions in one data set.

## Pretest

In this section, we will organize the combined, csv files to show all of the participants' pretest multi-choice responses. As the csv logged the answers through a string, we will create individual columns for each question while the rows will represent each randomized participant for analysis.

```{r}
# Filter the data to only focus on the pretest answers for each participant
pretest_data <- data_sampled[
  data_sampled$task == "pretest" &
    data_sampled$trial_type == "survey-multi-choice",
]
# Make the questions into each individual column so that we can score them
one_json <- pretest_data$response[1]
one_parsed <- fromJSON(one_json)
one_parsed
str(one_parsed)
## Create a list of all the JSON responses from the pretest section
pretest_list <- lapply(pretest_data$response, fromJSON)
## Combine the list of each individual participant into one data frame
pretest_answers <- do.call(
  rbind,
  lapply(pretest_list, as.data.frame)
)
head(pretest_answers)
names(pretest_answers)
# Ensure that all 80 participants are formatted into the answers
pretest_all_data <- dplyr::bind_cols(
  pretest_data %>%
    dplyr::select(participant_id, source_file, condition),
  pretest_answers
)
nrow(pretest_all_data)
head(pretest_all_data)
```

## Posttest

In this section, we will organize the combined, csv files to show all of the participants' posttest multi-choice responses. As the csv logged the answers through a string, we will create individual columns for each question while the rows will represent each randomized participant for analysis.

```{r}
# Keep only posttest MC rows for the 80 sampled participants
posttest_mc <- data_sampled %>%
  filter(phase == "mc")
# 2) Filter the columns to include the source file, item, condition, and response
posttest_long <- posttest_mc %>%
  select(participant_id, source_file, condition, item, response)
# Rearrange the format of the data set to cleanly show the answers for all 80 participants
posttest_all_data <- posttest_long %>%
  pivot_wider(
    names_from  = item,        
    values_from = response,   
    names_prefix = "mc_"     
  )
# Look at the data
nrow(posttest_all_data)
head(posttest_all_data)
```

# Scoring Study

In this section, we will try to score the pretest and posttest section before we run a descriptive statistics. Specifically, we will analyze both the pretest and posttest multi-choice answers.

## Pretest Accuracy

```{r}
# Define pretest CT skill questions
pretest_accuracy_questions <- c("Q0", "Q2", "Q4", "Q6", "Q8", "Q10", "Q12", "Q14")
# Pretest answer key
pretest_accuracy_key <- list(
  Q0 = "No correct conclusion possible",
  Q2 = "X and 7",
  Q4 = "Maybe they would be better off buying the Audi",
  Q6 = "No correct conclusion possible",
  Q8 = "Karin works at a bank",
  Q10 = "Less than 3%",
  Q12 = "15 and cola",
  Q14 = "The students score at least 0.1 point higher on the exam"
)
# Now, select the questions that are only accuracy_questions
pretest_accuracy_questions <- names(pretest_accuracy_key)
# Compare the answers to the key. Correct is true, incorrect is false!
correct_matrix <- as.data.frame(
  lapply(pretest_accuracy_questions, function(q) {
    pretest_all_data[[q]] == pretest_accuracy_key[[q]]
  })
)
# Convert the true/false into binary numbers (1 and 0)
correct_matrix <- correct_matrix * 1L
names(correct_matrix) <- paste0(pretest_accuracy_questions, "_score")
# Obtain both the total number correct per participant and the proportion correct
pretest_total_accuracy <- rowSums(correct_matrix, na.rm = TRUE)
pretest_total_prop    <- pretest_total_accuracy / length(pretest_accuracy_questions)
# Combine the total scores for all participants
pretest_scored <- cbind(
  pretest_all_data,
  correct_matrix,
  pretest_total_accuracy = pretest_total_accuracy,
  pretest_total_prop     = pretest_total_prop
)
# Look at the pretest scored data set
head(pretest_scored[
  , c("pretest_total_accuracy",
      "pretest_total_prop",
      "Q0_score", "Q2_score", "Q4_score")
])
```

## Pretest Mental Effort

```{r}
# Define pretest mental effort questions
effort_questions <- c("Q1", "Q3", "Q5", "Q7", "Q9", "Q11", "Q13", "Q15")
# Mental effort key. 
effort_key <- c(
  "Very little effort" = 1,
  "Little effort" = 2,
  "Not a little nor a lot of effort" = 3,
  "Quite a lot of effort" = 4,
  "A lot of effort" = 5
)
# Now, select the questions that are only effort_questions
effort_data <- pretest_answers[effort_questions]
# Convert the answers selected into the numbers in the effort_key
effort_key_number <- as.data.frame(
  lapply(effort_data, function(x) effort_key[x])
)
# Look at the data!!
head(effort_key_number)
summary(pretest_answers$pretest_effort)
```

## Posttest Accuracy

```{r}
# Posttest answer key
posttest_accuracy_key <- list(
  mc_q1 = "No correct conclusion possible",
  mc_q2 = "Perhaps give preference to graduates of Tilburg University.",
  mc_q3 = "No correct conclusion possible",
  mc_q4 = "Hearts and 2",
  mc_q5 = "Jacques is a janitor",
  mc_q6 = "Less than 10%",
  mc_q7 = "Madrid and Transavia",
  mc_q8 = "Within 5 years, the number of visitors will increase by 2%"
)
posttest_accuracy_questions <- names(posttest_accuracy_key)
# Compare answers to key → TRUE/FALSE
correct_matrix_post <- as.data.frame(
  lapply(posttest_accuracy_questions, function(q) {
    grepl(posttest_accuracy_key[[q]],
          posttest_all_data[[q]],
          fixed = TRUE)      # literal match, no regex weirdness
  })
)
# Convert the TRUE/FALSE answers 
correct_matrix_post <- correct_matrix_post * 1L
names(correct_matrix_post) <- paste0(posttest_accuracy_questions, "_score")
# Commpute total scores correct
posttest_total_accuracy <- rowSums(correct_matrix_post, na.rm = TRUE)
posttest_total_prop    <- posttest_total_accuracy / length(posttest_accuracy_questions)
# Combine the scoring all together with the data set
posttest_scored <- cbind(
  posttest_all_data[, c("participant_id", "source_file", "condition")],
  correct_matrix_post,
  posttest_total_accuracy = posttest_total_accuracy,
  posttest_total_prop     = posttest_total_prop
)
head(posttest_scored)
```

# Descriptive Statistics

\[WRITE INTRODUCTION\]

## Before Analysis – Formatting

Before running the descriptive statistics, confirmatory, and exploratory analyses, we will first combine the accuracy data set and format it for the different statistical studies to be ran smoothly.

### Combined Scored Data Set

Before we analyze the data, we will combine the pretest and posttest scored data. The logic of this section is combining the pretest and posttest scores in one data set. The scores will use the total accuracy score, as everyone completed 8 questions which we hope to identify how much they improved.

```{r}
# Combine the pretest and posttest scorred data into one data set
analysis_df <- pretest_scored %>%
  select(
    participant_id,
    condition,
    pretest = pretest_total_accuracy
  ) %>%
  left_join(
    posttest_scored %>%
      select(
        participant_id,
        posttest = posttest_total_accuracy
      ),
    by = "participant_id"
  )
# Open the combined, scored data of the pretest and posttest
head(analysis_df)
summary(analysis_df)
```

### Data Preparation for ANOVA

Next, we are going to create a data-set interpretable for ANOVA analyses. In this section, we will have the participant pretest and posttest paired with each other and accuracy shown side by side. Structuring the data in this fashion will be interpretable for both ggplots and ANOVA.

```{r}
# Pull the columns that we want to add for the new data set used for the ANOVA analyses
analysis_long <- analysis_df %>%
  pivot_longer(
    cols = c(pretest, posttest),
    names_to = "time",
    values_to = "accuracy"
  ) %>%
  mutate(
    time = factor(time, levels = c("pretest", "posttest")),
    condition = factor(condition)
  )
# Look at the data set
head(analysis_long)
names(analysis_long)
```

## Overview Statistics

In the overview, we will try to analyze the overall descriptive statistics of posttest and pretest total accuracy means and standard deviations. First, we are going to look at the total sample size we collected as well as per condition.

```{r}
# Report the total number of N 
overall_n <- data.frame(
  condition = "Overall",
  n = nrow(analysis_df)
)
# Report the total number of N per condition
by_condition_n <- as.data.frame(table(analysis_df$condition))
names(by_condition_n) <- c("condition", "n")
by_condition_n
# Combine
n_table <- bind_rows(overall_n, by_condition_n)
n_table
```

Afterwards, we are going to run the full descriptive statistics that will report the mean & sd overall as well as per condition.

```{r}
# Overall descriptive statistics
options(digits = 15)
overall_desc <- analysis_df %>%
  summarise(
    n = nrow(.),
    pretest_mean  = mean(pretest_total_accuracy, na.rm = TRUE),
    pretest_sd    = sd(pretest_total_accuracy, na.rm = TRUE),
    posttest_mean = mean(posttest_total_accuracy, na.rm = TRUE),
    posttest_sd   = sd(posttest_total_accuracy, na.rm = TRUE)
  )

overall_desc
# Descripitive statistics per condition
condition_desc <- analysis_df %>%
  dplyr::group_by(condition) %>%
  dplyr::summarise(
    n             = dplyr::n(),
    pretest_mean  = mean(pretest,  na.rm = TRUE),
    pretest_sd    = sd(pretest,    na.rm = TRUE),
    posttest_mean = mean(posttest, na.rm = TRUE),
    posttest_sd   = sd(posttest,   na.rm = TRUE),
    .groups = "drop"
  )

condition_desc
```

## Visualization of Overview Stats

In this section, we will be looking at a visualization of the mean pretest versus posttest accuracy by condition. Want to analyze which one improved more visually and look through the patterns before completing the confirmatory analysis to understand the main effects.

```{r}
# Creating a ggplot to see the mean pretest versus posttest accuracy by condition (SE vs no SE)
ggplot(analysis_long, aes(
  x = time,
  y = accuracy,
  group = condition,
  color = condition
)) +
  stat_summary(fun = mean, geom = "line", linewidth = 1) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  scale_color_manual(values = c("#004aad", "#FF9F45")) +  # academic colors
  theme_classic(base_size = 14) +
  labs(
    x = "Time",
    y = "Mean Accuracy",
    color = "Condition",
    title = "Mean Pretest vs Posttest Accuracy by Condition"
  )
```

# Confirmatory Analyses

For the confirmatory analyses, we will use a 2 X 2 mixed ANOVA to analyze our results between prior knowledge (pretest vs posttest) as the within-subjects factor and instructional condition (self-explanation versus no self-explanation) as the between-subjects variable. We hope to assess whether critical thinking performance is improved by integrating self-explanation combined with instructional guidance or not.

## Mixed ANOVA Analysis

In this section, we will run the main effect we want to see which is the interaction between condition (no SE versus SE) and timing (pretest versus posttest).

```{r}
anova_afex <- aov_ez(
  id = "participant_id",
  dv = "accuracy",
  data = analysis_long,
  within = "time",
  between = "condition",
  type = 3
)
# Show the ANOVA analysis!
anova_afex
```

```{r}
# Conduct a pairwise analysis to se
emm <- emmeans(anova_afex, ~ time * condition)
pairs(emm)
```

This analysis differs from the original replication project, that did a 3 X 2 X 2 analyses comparing the types of critical thinking questions. Additionally, they added a delayed posttest to measure the retention of this effect. This replication project truncated the analysis to ensure that people are writing the

## Visualization of Confirmatory Analysis

Showing several visualizations of the mixed ANOVA results.

### Interaction Plot

```{r}
anova_plot <- ggplot(
  analysis_long,
  aes(x = time, y = accuracy, color = condition, group = condition)
) +
  stat_summary(fun = mean, geom = "line", size = 1) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.1) +
  scale_color_manual(values = c("#004AAD", "#FF9F45")) +  # blue + orange
  theme_minimal(base_size = 14) +
  labs(
    title = "Interaction Plot: Accuracy by Time and Condition",
    x = "Time",
    y = "Accuracy",
    color = "Condition"
  )
# Add significance bar (adjust y values if needed to sit above your data)
anova_plot +
  annotate(
    "segment",
    x = 1, xend = 2,
    y = 3.25, yend = 3.25,
    color = "#FF9F45",
    size = 0.8
  ) +
  annotate(
    "text",
    x = 1.5, y = 3.32,
    label = "**",
    color = "#FF9F45",
    size = 6
  )
```

# Exploratory Analyses

The results reveal that
