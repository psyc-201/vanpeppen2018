---
title: "Replication of Vanpeppen et al. (2018) Data Analysis"
author: "Christine Lee"
date: "last updated at 11.29.2025"
format: 
  html:
    code-fold: false
    toc: true
    toc-depth: 3
    theme: cosmo
editor: visual
---

# Introduction

\[add brief introduction of what the project will be about\]

# Setup

```{r}
# Load required packages
library(tidyverse) 
library(knitr)
library(tidyr)
library(dplyr)
library(kableExtra)
library(jsonlite)
library(ez)
library(ggplot2)

# Set theme for plots
theme_set(theme_minimal(base_size = 12))

# Set random seed for reproducibility
set.seed(123)
```

# Data Loading

In this section, we will load in the file that holds all the SONA data. Then, we will merge the csv files together to interpret.

```{r}
# Labeling the pilot data & listing them for R to see
data_folder <- "/Users/christinelee/Documents/vanpeppen2018/sona_data"
list.files(data_folder)
# Make sure that all the csv files are in a list
csv_files <- list.files(
  data_folder,
  pattern = "\\.csv$",
  full.names = TRUE
)
# Check that it actually listed the amount of csv files in folder
length(csv_files)
# Load all the pilot data csvs into one dataset
all_data <- do.call(
  rbind,
  lapply(csv_files, function(file_path) {
    temp <- read.csv(file_path)
    temp$source_file <- basename(file_path)
    temp
  })
)
# Show a preview of the loaded, merged data
head(all_data)
```

# Pre-processing Data

Once the data in the file are all merged together, now we need to filter out the participants that failed the attention check.

```{r}
# Reload merged data
data_base <- all_data
# Score attention checks as true/false
data_base$attention_question_yes <-
  data_base$attention_question_passed == "true"
data_base$attention_effort_yes <-
  data_base$attention_effort_passed == "true"
# Filter the rows where the attention checks are true
passing_rows <- data_base [
  data_base$attention_question_yes == TRUE &
    data_base$attention_effort_yes == TRUE,
]
# Check how many rows passed the attention check
nrow(passing_rows)
# Look at the csv files that passed the attention checks
head(passing_rows[, c("source_file",
                      "attention_question_passed",
                      "attention_effort_passed",
                      "attention_question_yes",
                      "attention_effort_yes")])

# Select the source files that participants' passed.
passed_ids <- unique(passing_rows$source_file)
passed_ids
length(passed_ids)
# Finally, keep all the rows except those who have failed the attention check
data_attention <- data_base [
  data_base$source_file %in% passed_ids,
]
```

# Format Questions

In this section, I am going to let format the csv file to ensure the answers can be interprettable for scoring and analysis.

## Pretest

```{r}
# Filter the data to only focus on the pretest answers for each participant
pretest_data <- data_attention[
  data_attention$task == "pretest" &
    data_attention$trial_type == "survey-multi-choice",
]
# Make the questions into each individual column so that we can score them
one_json <- pretest_data$response[1]
one_parsed <- fromJSON(one_json)
one_parsed
str(one_parsed)
# Now make sure all participants are put in the question columns to score
## Create a list of all the JSON responses from the pretest section
pretest_list <- lapply(pretest_data$response, fromJSON)
## Combine the list of each individual participant into one data frame
pretest_answers <- do.call(
  rbind,
  lapply(pretest_list, as.data.frame)
)
head(pretest_answers)
names(pretest_answers)
# Ensure that all 80 participants are formatted into the answers
pretest_all_data <- dplyr::bind_cols(
  pretest_data %>%
    dplyr::select(participant_id, source_file, condition),
  pretest_answers
)
nrow(pretest_all_data)
head(pretest_all_data)
```

## Posttest

```{r}
# Filter the data to only MC posttest responses
posttest_mc <- data_attention %>%
  filter(phase == "mc")
# Parse JSON for each row
posttest_list <- lapply(posttest_mc$response, fromJSON)
# Combine each participant's JSON into one data frame
posttest_answers <- dplyr::bind_rows(
  lapply(posttest_list, as.data.frame)
)
# Attach participant metadata so you know whose answers are whose
posttest_answers_full <- dplyr::bind_cols(
  posttest_mc %>%
    select(participant_id, source_file, condition, item),
  posttest_answers
)
# Combine to one row per participant (per file), wide format
posttest_mc_wide <- posttest_answers_full %>%
  group_by(participant_id, source_file, condition) %>%
  summarise(
    across(ends_with("_mc"), ~ .x[!is.na(.x)][1], .names = "{.col}"),
    .groups = "drop"
  )
glimpse(posttest_mc_wide)
head(posttest_mc_wide)
# Ensure that all 80 participants are accounted in the data
posttest_all_data <- posttest_mc_wide
nrow(posttest_all_data)
head(posttest_all_data)
```

# Scoring Study

In this section, we will try to score the pretest and posttest section before we run a descriptive statistics. Main areas of scoring will be in the pretest

## Pretest Accuracy

```{r}
# Define pretest CT skill questions
pretest_accuracy_questions <- c("Q0", "Q2", "Q4", "Q6", "Q8", "Q10", "Q12", "Q14")
# Pretest answer key
pretest_accuracy_key <- list(
  Q0 = "No correct conclusion possible",
  Q2 = "X and 7",
  Q4 = "Maybe they would be better off buying the Audi",
  Q6 = "No correct conclusion possible",
  Q8 = "Karin works at a bank",
  Q10 = "Less than 3%",
  Q12 = "15 and cola",
  Q14 = "The students score at least 0.1 point higher on the exam"
)
# Now, select the questions that are only accuracy_questions
pretest_accuracy_questions <- names(pretest_accuracy_key)
# Compare the answers to the key. Correct is true, incorrect is false!
correct_matrix <- as.data.frame(
  lapply(pretest_accuracy_questions, function(q) {
    pretest_all_data[[q]] == pretest_accuracy_key[[q]]
  })
)
# Convert the true/false into binary numbers (1 and 0)
correct_matrix <- correct_matrix * 1L
names(correct_matrix) <- paste0(pretest_accuracy_questions, "_score")
# Obtain both the total number correct per participant and the proportion correct
pretest_total_accuracy <- rowSums(correct_matrix, na.rm = TRUE)
pretest_total_prop    <- pretest_total_accuracy / length(pretest_accuracy_questions)
# Combine the total scores for all participants
pretest_scored <- cbind(
  pretest_all_data,
  correct_matrix,
  pretest_total_accuracy = pretest_total_accuracy,
  pretest_total_prop     = pretest_total_prop
)
# Look at the pretest scored data set
head(pretest_scored[
  , c("pretest_total_accuracy",
      "pretest_total_prop",
      "Q0_score", "Q2_score", "Q4_score")
])
```

## Pretest Mental Effort

```{r}
# Define pretest mental effort questions
effort_questions <- c("Q1", "Q3", "Q5", "Q7", "Q9", "Q11", "Q13", "Q15")
# Mental effort key. 
effort_key <- c(
  "Very little effort" = 1,
  "Little effort" = 2,
  "Not a little nor a lot of effort" = 3,
  "Quite a lot of effort" = 4,
  "A lot of effort" = 5
)
# Now, select the questions that are only effort_questions
effort_data <- pretest_answers[effort_questions]
# Convert the answers selected into the numbers in the effort_key
effort_key_number <- as.data.frame(
  lapply(effort_data, function(x) effort_key[x])
)
# Look at the data!!
head(effort_key_number)
summary(pretest_answers$pretest_effort)
```

## Posttest Accuracy

```{r}
# Posttest answer key
posttest_accuracy_key <- list(
  q1_mc = "No correct conclusion possible",
  q2_mc = "Perhaps give preference to graduates of Tilburg University.",
  q3_mc = "No correct conclusion possible",
  q4_mc = "Hearts and 2",
  q5_mc = "Jacques is a janitor",
  q6_mc = "Less than 10%",
  q7_mc = "Madrid and Transavia",
  q8_mc = "Within 5 years, the number of visitors will increase by 2%"
)
# Now, select the questions that are only accuracy_questions
posttest_accuracy_questions <- names(posttest_accuracy_key)
# Compare the answers to the key. Correct is true, incorrect is false!
correct_matrix_post <- as.data.frame(
  lapply(posttest_accuracy_questions, function(q) {
    posttest_all_data[[q]] == posttest_accuracy_key[[q]]
  })
)
# Convert the true/false into binary numbers (1 and 0)
correct_matrix_post <- correct_matrix_post * 1L
names(correct_matrix_post) <- paste0(posttest_accuracy_questions, "_score")
# Compute total score correct
posttest_total_accuracy <- rowSums(correct_matrix_post, na.rm = TRUE)
posttest_total_prop    <- posttest_total_accuracy / length(posttest_accuracy_questions)
# Combine the total scores for all participants
posttest_scored <- cbind(
  posttest_all_data,
  correct_matrix_post,
  posttest_total_accuracy = posttest_total_accuracy,
  posttest_total_prop     = posttest_total_prop
)
# Run the data set
head(posttest_scored[
  , c("posttest_total_accuracy",
      "posttest_total_prop",
      paste0(posttest_accuracy_questions[1], "_score"),
      paste0(posttest_accuracy_questions[2], "_score"),
      paste0(posttest_accuracy_questions[3], "_score"))
])
```

# Descriptive Statistics

\[WRITE INTRODUCTION\]

## Before Analysis â€“ Formatting

Before running the descriptive statistics, confirmatory, and exploratory analyses, we will first combine the accuracy data set and format it for the different statistical studies to be ran smoothly.

### Combined Scored Data Set

Before we analyze the data, we will combine the pretest and posttest scored data. The logic of this section is combining the pretest and posttest scores in one data set. The scores will use the total accuracy score, as everyone completed 8 questions which we hope to identify how much they improved.

```{r}
# Combine the pretest and posttest scorred data into one data set
analysis_df <- pretest_scored %>%
  select(
    participant_id,
    condition,
    pretest = pretest_total_accuracy
  ) %>%
  left_join(
    posttest_scored %>%
      select(
        participant_id,
        posttest = posttest_total_accuracy
      ),
    by = "participant_id"
  )
# Open the combined, scored data of the pretest and posttest
head(analysis_df)
summary(analysis_df)
```

### Data Preparation for ANOVA

Next, we are going to create a data-set interpretable for ANOVA analyses. In this section, we will have the participant pretest and posttest paired with each other and accuracy shown side by side. Structuring the data in this fashion will be interpretable for both ggplots and ANOVA.

```{r}
# Pull the columns that we want to add for the new data set used for the ANOVA analyses
analysis_long <- analysis_df %>%
  pivot_longer(
    cols = c(pretest, posttest),
    names_to = "time",
    values_to = "accuracy"
  ) %>%
  mutate(
    time = factor(time, levels = c("pretest", "posttest")),
    condition = factor(condition)
  )
# Look at the data set
head(analysis_long)
names(analysis_long)
```

## Overview Statistics

In the overview, we will try to analyze the overall descriptive statistics of posttest and pretest total accuracy means and standard deviations.

```{r}
# Total number of N as well as N per condition
analysis_df %>%
  count(condition, name = "n")
# Overall descriptive statistics 
analysis_df %>%
  summarise(
    pretest_mean  = mean(pretest_total_accuracy, na.rm = TRUE),
    pretest_sd    = sd(pretest_total_accuracy, na.rm = TRUE),
    posttest_mean = mean(posttest_total_accuracy, na.rm = TRUE),
    posttest_sd   = sd(posttest_total_accuracy, na.rm = TRUE)
  )
# Descriptive statistic overview of each condition
analysis_df %>%
  group_by(condition) %>%
  summarise(
    n             = n(),
    pretest_mean  = mean(pretest, na.rm = TRUE),
    pretest_sd    = sd(pretest, na.rm = TRUE),
    posttest_mean = mean(posttest, na.rm = TRUE),
    posttest_sd   = sd(posttest, na.rm = TRUE)
  )
```

## Visualization of Overview Stats

In this section, we will be looking at a visualization of the mean pretest versus posttest accuracy by condition. Want to analyze which one improved more visually and look through the patterns before completing the confirmatory analysis to understand the main effects.

```{r}
# Creating a ggplot to see the mean pretest versus posttest accuracy by condition (SE vs no SE)
ggplot(analysis_long, aes(
  x = time,
  y = accuracy,
  group = condition,
  color = condition
)) +
  stat_summary(fun = mean, geom = "line", linewidth = 1) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  labs(
    x = "Time",
    y = "Mean Accuracy",
    color = "Condition",
    title = "Mean Pretest vs Posttest Accuracy by Condition"
  ) +
  theme_minimal(base_size = 14)
```

# Confirmatory Analyses

For the confirmatory analyses, we will use a 2 X 2 mixed ANOVA to analyze our results between prior knowledge (pretest vs posttest) as the within-subjects factor and instructional condition (self-explanation versus no self-explanation) as the between-subjects variable. We hope to assess whether critical thinking performance is improved by integrating self-explanation combined with instructional guidance or not.

## Mixed ANOVA Analysis

In this section, we will run the main effect we want to see which is the interaction between condition (no SE versus SE) and timing (pretest versus posttest).

```{r}
anova_afex <- aov_ez(
  id = "participant_id",
  dv = "accuracy",
  data = analysis_long,
  within = "time",
  between = "condition",
  type = 3
)
# Show the ANOVA analysis!
anova_afex
```

This analysis differs from the original replication project, that did a 3 X 2 X 2 analyses comparing the types of critical thinking questions. Additionally, they added a delayed posttest to measure the retention of this effect. This replication project truncated the analysis to ensure that people are writing the

## Pretest & Mental Ratings Comparison

```{r}

```
