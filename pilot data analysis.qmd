---
title: "Replication of Vanpeppen et al. (2018) Data Analysis"
author: "Christine Lee"
date: "last updated at 11.29.2025"
format: 
  html:
    code-fold: false
    toc: true
    toc-depth: 3
    theme: cosmo
editor: visual
---

# Introduction

\[add brief introduction of what the project will be about\]

# Setup

```{r}
# Load required packages
library(tidyverse) 
library(knitr)
library(kableExtra)
library(jsonlite)

# Set theme for plots
theme_set(theme_minimal(base_size = 12))

# Set random seed for reproducibility
set.seed(123)
```

# Data Loading

In this section, we will load in the file that holds all the SONA data. Then, we will merge the csv files together to interpret.

```{r}
# Labeling the pilot data & listing them for R to see
data_folder <- "/Users/christinelee/Documents/vanpeppen2018/sona_data_pt1"
list.files(data_folder)
# Make sure that all the csv files are in a list
csv_files <- list.files(
  data_folder,
  pattern = "\\.csv$",
  full.names = TRUE
)
# Check that it actually listed the amount of csv files in folder
length(csv_files)
# Load all the pilot data csvs into one dataset
all_data <- do.call(
  rbind,
  lapply(csv_files, function(file_path) {
    temp <- read.csv(file_path)
    temp$source_file <- basename(file_path)
    temp
  })
)
# Show a preview of the loaded, merged data
head(all_data)
```

# Pre-processing Data

Once the data in the file are all merged together, now we need to filter out the participants that failed the attention check.

```{r}
# Reload merged data
data_base <- all_data
# Score attention checks as true/false
data_base$attention_question_yes <-
  data_base$attention_question_passed == "true"
data_base$attention_effort_yes <-
  data_base$attention_effort_passed == "true"
# Filter the rows where the attention checks are true
passing_rows <- data_base [
  data_base$attention_question_yes == TRUE &
    data_base$attention_effort_yes == TRUE,
]
# Check how many rows passed the attention check
nrow(passing_rows)
# Look at the csv files that passed the attention checks
head(passing_rows[, c("source_file",
                      "attention_question_passed",
                      "attention_effort_passed",
                      "attention_question_yes",
                      "attention_effort_yes")])

# Select the source files that participants' passed.
passed_ids <- unique(passing_rows$source_file)
passed_ids
length(passed_ids)
# Finally, keep all the rows except those who have failed the attention check
data_attention <- data_base [
  data_base$source_file %in% passed_ids,
]
```

# Format Questions

In this section, I am going to let format the csv file to ensure the answers can be interprettable for scoring and analysis.

## Pretest 

```{r}
# Filter the data to only focus on the pretest answers for each participant
pretest_data <- data_attention[
  data_attention$task == "pretest" &
    data_attention$trial_type == "survey-multi-choice",
]
# Make the questions into each individual column so that we can score them
one_json <- pretest_data$response[1]
one_parsed <- fromJSON(one_json)
one_parsed
str(one_parsed)
# Now make sure all participants are put in the question columns to score
## Create a list of all the JSON responses from the pretest section
pretest_list <- lapply(pretest_data$response, fromJSON)
## Combine the list of each individual participant into one data frame
pretest_answers <- do.call(
  rbind,
  lapply(pretest_list, as.data.frame)
)
head(pretest_answers)
names(pretest_answers)
```

## Posttest

```{r}
# Filter the data to only MC posttest responses
posttest_mc <- posttest_raw %>%
  filter(phase == "mc")
# Parse JSON for each row
posttest_list <- lapply(posttest_mc$response, fromJSON)
# Combine each participant's JSON into one data frame
posttest_answers <- dplyr::bind_rows(
  lapply(posttest_list, as.data.frame)
)
# Attach participant metadata so you know whose answers are whose
posttest_answers_full <- dplyr::bind_cols(
  posttest_mc %>%
    select(participant_id, source_file, condition, item),
  posttest_answers
)
# Combine to one row per participant (per file), wide format
posttest_mc_wide <- posttest_answers_full %>%
  group_by(participant_id, source_file, condition) %>%
  summarise(
    across(ends_with("_mc"), ~ .x[!is.na(.x)][1], .names = "{.col}"),
    .groups = "drop"
  )
glimpse(posttest_mc_wide)
head(posttest_mc_wide)
```

# Scoring Study

In this section, we will try to score the pretest and posttest section before we run a descriptive statistics. Main areas of scoring will be in the pretest

## Pretest Accuracy

```{r}
# Define pretest CT skill questions
pretest_accuracy_questions <- c("Q0", "Q2", "Q4", "Q6", "Q8", "Q10", "Q12", "Q14")
# Pretest answer key
pretest_accuracy_key <- list(
  Q0 = "No correct conclusion possible",
  Q2 = "X and 7",
  Q4 = "Maybe they would be better off buying the Audi",
  Q6 = "No correct conclusion possible",
  Q8 = "Karin works at a bank",
  Q10 = "Less than 3%",
  Q12 = "15 and cola",
  Q14 = "The students score at least 0.1 point higher on the exam"
)
# Now, select the questions that are only accuracy_questions
pretest_accuracy_questions <- names(accuracy_key)
# Compare the answers to the key. Correct is true, incorrect is false!
correct_matrix_pre <- pretest_answers[accuracy_questions] == accuracy_key
# Convert the true/false into binary numbers (1 and 0)
correct_matrix <- correct_matrix * 1
pretest_answers$pretest_accuracy <- rowSums(correct_matrix)
# Obtain both the total number correct per participant and the proportion correct
pretest_total_accuracy <- rowSums(correct_matrix_pre, na.rm = TRUE)
pretest_total_prop <- pretest_total_accuracy / length(pretest_accuracy_questions)
# Combine the total scores for all participants
pretest_all_data <- pretest_data[, c("participant_id", "source_file", "condition")]
pretest_scored <- cbind(
  pretest_all_data,
  correct_matrix_pre,         
  pretest_total_accuracy,
  pretest_total_prop
)
# Run the dataset
head(pretest_scored)
```

## Pretest Mental Effort

```{r}
# Define pretest mental effort questions
effort_questions <- c("Q1", "Q3", "Q5", "Q7", "Q9", "Q11", "Q13", "Q15")
# Mental effort key. 
effort_key <- c(
  "Very little effort" = 1,
  "Little effort" = 2,
  "Not a little nor a lot of effort" = 3,
  "Quite a lot of effort" = 4,
  "A lot of effort" = 5
)
# Now, select the questions that are only effort_questions
effort_data <- pretest_answers[effort_questions]
# Convert the answers selected into the numbers in the effort_key
effort_key_number <- as.data.frame(
  lapply(effort_data, function(x) effort_key[x])
)
# Look at the data!!
head(effort_key_number)
summary(pretest_answers$pretest_effort)
```

## Posttest Accuracy

```{r}
# Posttest answer key
posttest_accuracy_key <- list(
  q1_mc = "No correct conclusion possible",
  q2_mc = "Perhaps give preference to graduates of Tilburg University.",
  q3_mc = "No correct conclusion possible",
  q4_mc = "Hearts and 2",
  q5_mc = "Jacques is a janitor",
  q6_mc = "Less than 10%",
  q7_mc = "Madrid and Transavia",
  q8_mc = "Within 5 years, the number of visitors will increase by 2%"
)
# Now, select the questions that are only accuracy_questions
accuracy_questions_post <- names(posttest_accuracy_key)
# Compare the answers to the key. Correct is true, incorrect is false!
correct_matrix_post <- posttest_mc_wide[accuracy_questions_post] == posttest_accuracy_key
# Convert the true/false into binary numbers (1 and 0)
correct_matrix_post_num <- correct_matrix_post * 1
# Compute total score correct
posttest_total_accuracy <- rowSums(correct_matrix_post_num, na.rm = TRUE)
posttest_total_prop <- posttest_total_accuracy / length(accuracy_questions_post)
# Combine the total scores for all participants
posttest_scored <- cbind(
  posttest_mc_wide[c("participant_id", "source_file", "condition")],
  correct_matrix_post_num,
  posttest_total_accuracy,
  posttest_total_prop
)
# Run the data set
head(posttest_scored)
```

# Descriptive Stats

Running a data analysis to understand whether this study replicated properly.

## Pretest versus Posttest Accuracy

In this section, I am going to run a 2 X 2 ANOVA analyses between SE/no SE condition & pretest/posttest.

```{r}
# 
full_scores$condition <- factor(full_scores$condition)
levels(full_scores$condition)


```

## Pretest & Mental Ratings Comparison

```{r}
# Make sure to link the
analysis_dr <- cbind(
  correct_matrix,
  effort_key_number
)
# Average the effort scores
pretest_answers$pretest_effort <- rowMeans(effort_key_number)
```
